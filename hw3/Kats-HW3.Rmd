---
title: "DATA 621 Homework 3"
author: "Ilya Kats"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(caTools)
library(pscl)
library(ROCR)
library(MASS)
library(caret)

opts_chunk$set(echo = TRUE)
```

## Summary

This report covers an attempt to build a binary logistic regression model to predict whether the crime rate is above the median crime rate. The model is based on a data set containing information on crime for various Boston neighborhoods. 

## Data Exploration

The data set includes 466 observations with 12 variables (excluding the target variable).

#### Summary of Variables

```{r echo=FALSE, warning=FALSE}
crime <- read.csv("crime-training-data_modified.csv")
sumCrime = data.frame(Variable = character(),
                      Min = integer(),
                      Median = integer(),
                      Mean = double(),
                      SD = double(),
                      Max = integer(),
                      Num_NAs = integer(),
                      Num_Zeros = integer())
for (i in 1:13) {
  sumCrime <- rbind(sumCrime, data.frame(Variable = colnames(crime)[i],
                                   Min = min(crime[,i], na.rm=TRUE),
                                   Median = median(crime[,i], na.rm=TRUE),
                                   Mean = mean(crime[,i], na.rm=TRUE),
                                   SD = sd(crime[,i], na.rm=TRUE),
                                   Max = max(crime[,i], na.rm=TRUE),
                                   Num_NAs = sum(is.na(crime[,i])),
                                   Num_Zeros = length(which(crime[,i]==0)))
                 )
}
colnames(sumCrime) <- c("Variable", "Min", "Median", "Mean", "SD", "Max", "Num of NAs", "Num of Zeros")
kable(sumCrime, row.names=FALSE)
```

#### Independent Variables

- `zn`- _proportion of residential land zoned for large lots (over 25,000 square feet)_ - 339 out of 466 (or about 76%) of observations have a value of 0. It is possible that majority of neighborhoods will not have any residential land zoned for large lots. Therefore, it is likely that 0 represents a valid value rather than a missing one. 
- `indus` - _proportion of non-retail business acres per suburb_
- `chas` - _a dummy variable for whether the suburb borders the Charles River (1) or not (0)_ - 433 out of 466 (or about 92.9%) of observations have a value of 0. Even though the Charles River is a promimnent feature of the Boston area, it is quite reasonable to assume that most neighborhoods do not border the river.
- `nox` - _nitrogen oxides concentration (parts per 10 million)_ - Looking at the scatterplot there seems to be some correlation between the nitrogen oxides concentration and the target variable.

```{r echo=FALSE, warning=FALSE}
ggplot(crime, aes(x=nox, y=target)) + 
  geom_point() +
  stat_smooth(method="glm", method.args=list(family="binomial"), se=FALSE)
```

- `rm` - _average number of rooms per dwelling_ - Because this is an average of number of rooms, this is a continous variable.
- `age` - _proportion of owner-occupied units built prior to 1940_ - There is nothing unusual about this variable; however, it is interesting to note that the mean of 68.37 and median of 77.15 shows that there a large number of pre-war buildings. Not surprising for an old city like Boston.
- `dis` - _weighted mean of distances to five Boston employment centers_ - Interestingly, majority of observations above the median crime rate are within 5 miles of an employment center (there are only 2 observations over 5 miles away). And there may be some correlation between distance and the target variable.
- `rad` - _index of accessibility to radial highways_ - This is a discrete variable with 9 different values in the observations (1 through 8 and 24).
- `tax` - _full-value property-tax rate per $10,000_
- `ptratio` - _pupil-teacher ratio by town_
- `lstat` - _lower status of the population (percent)_
- `medv` - _median value of owner-occupied homes in \$1000s_

#### Target/Dependent Variable

- `target` - _whether the crime rate is above the median crime rate (1) or not (0)_ - There are 237 observation with `target` value of 0 and 229 observations with `target` value of 1 making it about 50/50 split, or more precisely there are **50.86% of 0s and 49.14% of 1s**.

#### Correlation Matrix

Below is the correlation matrix for the data set. 

- There is a very high correlation (0.91) between `tax` and `rad`. It seems that the higher the highway accessibility is, the higher property taxes are. Alternatively, radial highways and higher property taxes may signify suburbs while lack of radial highways may imply inner city (with possibly poorer, lower taxed properties).
- `nox` has the highest correlation with the target variable, but `age`, `dis`, `rad` and `tax` are also fairly highly correlated to `target` (above 0.6).
- The following pairs have correlation at or above 0.7 (or below -0.7 in case of negative correlation): `nox`/`indus`, `dis`/`indus`, `tax`/`indus`,  `age`/`nox`, `dis`/`nox`, `medv`/`rm`, `dis`/`age` and `medv`/`lstat`.

```{r echo=FALSE, warning=FALSE}
cm <- cor(crime, use="pairwise.complete.obs")
cm <- round(cm, 2)
cmout <- as.data.frame(cm) %>% mutate_all(function(x) {
  cell_spec(x, "html", color = ifelse(x>0.69 | x<(-0.69),"blue","black"))
  })
rownames(cmout) <- colnames(cmout)
cmout %>%
  kable("html", escape = F, align = "c", row.names = TRUE) %>%
  kable_styling("striped", full_width = F)
```

#### Scatterplot Matrix

```{r echo=FALSE, warning=FALSE}
pairs(crime)
```

- Reviewing the scatterplot matrix shows several pairs with possible relationships. Two most prominent are `nox`/`dis` and `lstat`/`medv`. 
- `rm` and `medv` may have a linear relationship as well. 
- `rad` and `tax` have a very prominent outlier. Upon further inpection of data we can see that all observations with `rad` value of 24 have a `tax` value of 666, so the outlier is actually multiple observations mapped to the same spot. 

Above data analysis treats `rad` and `chas` as numeric variables; however, treating them as categorical variables may better reflect the nature of those variables, so for modelling they will be converted to factors. 

```{r echo=FALSE, warning=FALSE}
crime[,'rad'] <- as.factor(crime[,'rad'])
crime[,'chas'] <- as.factor(crime[,'chas'])
```

## Modelling

As the first step, in order to test and compare performance of various models, data was split into training (75%) and testing (25%) sets. The training set includes 350 randomly chosen observations and the testing set includes 116 remaining observations. 

```{r echo=FALSE, warning=FALSE}
set.seed(88)
split <- sample.split(crime$target, SplitRatio = 0.75)
crimeTRAIN <- subset(crime, split == TRUE)
crimeTEST <- subset(crime, split == FALSE)
```

#### Model 1: Variables with High Correlation to Target Variable

The first model includes 5 variables with the highest correlation coefficients when compared agains the target variable. This will allow the testing of model performance evaluation methods as well as corresponding R code.

```{r echo=FALSE, warning=FALSE}
model <- glm (target ~ nox+age+dis+rad+tax, data = crimeTRAIN, family = binomial(link="logit"))
summary(model)
pred <- predict(model, newdata=subset(crimeTEST, select=c(1:12)), type='response')
cm <- confusionMatrix(as.factor(crimeTEST$target), as.factor(ifelse(pred > 0.5,1,0)))
cm$table
accuracy <- cm$overall['Accuracy']
R2 <- pR2(model)["McFadden"]
```

Model summary and confusion matrix of running the model against test data are above. The accuracy rate (`r accuracy`) is very good and the McFadden R^2 value (`r R2`) is also high. AIC value is 123.66. Additionally, consider the ROC curve for this model.

```{r echo=FALSE, warning=FALSE}
pr <- prediction(pred, crimeTEST$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, colorize = TRUE, text.adj = c(-0.2,1.7))
auc <- performance(pr, measure = "auc")@y.values[[1]]
```

Area under the curve is `r auc`.

#### Model 2: All Variables

The second model includes all 12 available independent variables.

```{r echo=FALSE, warning=FALSE}
model <- glm (target ~ ., data = crimeTRAIN, family = binomial(link="logit"))
summary(model)
pred <- predict(model, newdata=subset(crimeTEST, select=c(1:12)), type='response')
cm <- confusionMatrix(as.factor(crimeTEST$target), as.factor(ifelse(pred > 0.5,1,0)))
cm$table
accuracy <- cm$overall['Accuracy']
R2 <- pR2(model)["McFadden"]
```

Model summary and confusion matrix of running the model against test data are above. The accuracy rate (`r accuracy`) is good and the McFadden R^2 value (`r R2`) is also high. AIC value is 128.1. Additionally, consider the ROC curve for this model.

```{r echo=FALSE, warning=FALSE}
pr <- prediction(pred, crimeTEST$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, colorize = TRUE, text.adj = c(-0.2,1.7))
auc <- performance(pr, measure = "auc")@y.values[[1]]
```

Area under the curve is `r auc`. 

Comparing to the first model AIC has slightly increased (worse), but accuracy, McFadden R^2 and AUC all also slightly increased (better). 

#### Model 3: _StepAIC_ Method

The third model starts with all 12 available independent variables, but then drops them one by one using the stepwise algorithm.

```{r echo=FALSE, warning=FALSE}
model <- glm (target ~ ., data = crimeTRAIN, family = binomial(link="logit"))
model <- stepAIC(model, trace=FALSE, direction="both")
summary(model)
pred <- predict(model, newdata=subset(crimeTEST, select=c(1:12)), type='response')
cm <- confusionMatrix(as.factor(crimeTEST$target), as.factor(ifelse(pred > 0.5,1,0)))
cm$table
accuracy <- cm$overall['Accuracy']
R2 <- pR2(model)["McFadden"]
```

Model summary and confusion matrix of running the model against test data are above. The accuracy rate (`r accuracy`) is very good and the McFadden R^2 value (`r R2`) is also high. AIC value is 118.99. Additionally, consider the ROC curve for this model.

```{r echo=FALSE, warning=FALSE}
pr <- prediction(pred, crimeTEST$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, colorize = TRUE, text.adj = c(-0.2,1.7))
auc <- performance(pr, measure = "auc")@y.values[[1]]
```

Area under the curve is `r auc`. 

The third model has the best (lowest) AIC value (better). Accuracy is the same as for the first model (and slightly lower than the second model). AUC is lower, but very close to the AUC value for the second model. Finally, McFadden R^2 falls between the first and second models, but the change is also very small. 

#### Additional Models

Basic models produced very efficient results. Several other models were attempted, but they did not produce significant improvements and therefore simplier, easier to interpret basic models were preferred. Other models included variable transformations and variable interactions. Since this project does not deals with critical and sensitive data with high cost of errors, such as medical or national security, the accuracy of the basic models is deemed acceptable.

## Model Selection

All 3 models generated good overall results, but the third model (_StepAIC_ model) is chosen for its simplicity. It is important to note that even though general parameters between models are close, one may be preferred over the other based on application. For example, the second and third models have similar number of errors (6 and 7); however, the second model has more Type II/False Negative errors and less Type I/False Positive errors than the third model. This difference in sensitivity and specificity may be important for some applications. 

```{r echo=FALSE, warning=FALSE}
model <- glm(formula = target ~ zn + nox + rad + tax, family = binomial(link = "logit"), data = crimeTRAIN)
```

## Model Performance

#### K-Fold Cross Validation

To assess the performance of selected model, below are results of 10-fold cross-validation. The model performs as expected.

```{r echo=FALSE, warning=FALSE}
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
model_fit <- train(target ~ zn + nox + rad + tax,  data=crimeTRAIN, 
                   method="glm", family="binomial",
                   trControl = ctrl, tuneLength = 5)
pred <- predict(model_fit, newdata=crimeTEST)
confusionMatrix(as.factor(crimeTEST$target), as.factor(ifelse(pred > 0.5,1,0)))
```

Similarly, the deviance table below demonstrated that each variable significantly contributes to the drop in deviance difference.

```{r echo=FALSE, warning=FALSE}
anova(model, test="Chisq")
```

## APPENDIX: R Script

```{r eval=FALSE}
```